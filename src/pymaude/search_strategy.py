# search_strategy.py - Reproducible device search strategies
# Copyright (C) 2026 Jacob Schwartz <jaschwa@umich.edu>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

"""
Reproducible device search strategies for MAUDE systematic reviews.

This module provides the DeviceSearchStrategy class for documenting,
versioning, and applying device search criteria following PRISMA 2020
and RECORD reporting guidelines.
"""

from dataclasses import dataclass, field, asdict
from typing import Optional, List, Dict, Tuple, Union, Any
from datetime import datetime
from pathlib import Path
import yaml
import pandas as pd


@dataclass
class DeviceSearchStrategy:
    """
    Encapsulates a reproducible search strategy for a device class.

    This class enables documentation of search criteria following PRISMA 2020
    and RECORD reporting guidelines for systematic reviews of administrative data.

    The search strategy tracks:
    - Boolean search criteria (broad and narrow searches)
    - Known device name variants for documentation
    - Exclusion patterns for false positives
    - Manual inclusion/exclusion overrides
    - Version history and rationale

    Attributes:
        name: Identifier for this search strategy (e.g., "rotational_thrombectomy")
        description: Human-readable description of device category
        version: Semantic version (e.g., "1.0.0")
        author: Strategy author name
        created_at: Creation timestamp (auto-generated)
        updated_at: Last modification timestamp (auto-generated)
        broad_criteria: Boolean search (list format) for initial broad search
        narrow_criteria: Boolean search (list format) for refined search
        known_variants: Device name variants for documentation
        exclusion_patterns: Known false positive patterns
        inclusion_overrides: MDR_REPORT_KEYs to force-include
        exclusion_overrides: MDR_REPORT_KEYs to force-exclude
        search_rationale: Documentation of why these criteria were chosen

    Examples:
        # Create strategy
        strategy = DeviceSearchStrategy(
            name="rotational_thrombectomy",
            description="Rotational thrombectomy devices",
            broad_criteria=[['argon', 'cleaner'], ['thrombectomy', 'rotational']],
            narrow_criteria=[['argon', 'cleaner', 'thromb'], ['rex', 'cleaner']],
            search_rationale="Focus on Argon Cleaner devices..."
        )

        # Apply to database
        included, excluded, needs_review = strategy.apply(db)

        # Save for reproducibility
        strategy.to_yaml('strategies/my_strategy.yaml')

        # Load existing strategy
        strategy = DeviceSearchStrategy.from_yaml('strategies/my_strategy.yaml')

    Note:
        Boolean search criteria support two formats:

        Standard (list) format:
        - OR: ['term1', 'term2']
        - AND: [['term1', 'term2']]
        - Complex: [['argon', 'cleaner'], 'angiojet']
          Equivalent to: (argon AND cleaner) OR angiojet

        Grouped (dict) format:
        - {'group1': criteria1, 'group2': criteria2}
        - Each group's criteria can use standard list format
        - Both broad_criteria and narrow_criteria must have matching group keys
        - Output DataFrames include search_group column for group membership
        - Use None as criteria to skip a group (match nothing for that group)
        - Example: {'mechanical': [['argon', 'cleaner']], 'aspiration': 'penumbra'}
        - Example with None: {'g1': [['term']], 'g2': None}  # g2 matches nothing

    References:
        PRISMA 2020: https://www.prisma-statement.org/
        RECORD: https://www.record-statement.org/
    """

    # Metadata
    name: str
    description: str
    version: str = "1.0.0"
    author: str = ""
    created_at: datetime = field(default_factory=datetime.now)
    updated_at: datetime = field(default_factory=datetime.now)

    # Search criteria (list, str, or dict format matching MaudeDatabase.search_by_device_names)
    broad_criteria: Union[List, str, Dict] = field(default_factory=list)
    narrow_criteria: Union[List, str, Dict] = field(default_factory=list)

    # Known device variants (for documentation/future fuzzy matching)
    known_variants: List[Dict[str, str]] = field(default_factory=list)
    # Format: [{"device_name": "...", "generic_name": "...", "manufacturer": "...", "canonical_id": "..."}]

    # Exclusion patterns (known false positives - substring matches)
    exclusion_patterns: List[str] = field(default_factory=list)

    # Manual overrides (MDR_REPORT_KEY strings)
    inclusion_overrides: List[str] = field(default_factory=list)
    exclusion_overrides: List[str] = field(default_factory=list)

    # Documentation
    search_rationale: str = ""

    def apply(self, db, name_column: str = "DEVICE_NAME_CONCAT",
              start_date: Optional[str] = None, end_date: Optional[str] = None) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """
        Apply search strategy to a MaudeDatabase.

        Workflow (following PRISMA):
        1. Broad search → candidate reports
        2. Narrow search → refined subset
        3. Calculate difference → reports needing manual review
        4. Apply exclusion patterns → remove false positives
        5. Apply manual overrides → honor adjudication decisions

        Args:
            db: MaudeDatabase instance
            name_column: Column for substring matching (default: DEVICE_NAME_CONCAT)
            start_date: Optional start date (YYYY-MM-DD)
            end_date: Optional end date (YYYY-MM-DD)

        Returns:
            Tuple of (included, excluded, needs_review) DataFrames:
            - included: Reports definitively included (narrow + manual inclusions)
            - excluded: Reports definitively excluded (false positives + manual exclusions)
            - needs_review: Reports requiring manual adjudication (broad - narrow - excluded)

            When using dict criteria (grouped search), all DataFrames include search_group column.

        Raises:
            ValueError: If search criteria are empty or invalid

        Note:
            Supports both list and dict criteria formats:
            - List format: [['term1', 'term2'], 'term3'] for standard searches
            - Dict format: {'group1': [...], 'group2': [...]} for grouped searches

            When using dict format, both broad_criteria and narrow_criteria must be dicts
            with matching group keys.
        """
        from .database import MaudeDatabase

        # Validate database instance
        if not isinstance(db, MaudeDatabase):
            raise ValueError(f"db must be a MaudeDatabase instance, got {type(db).__name__}")

        # Validate criteria
        if not self.broad_criteria:
            raise ValueError("broad_criteria cannot be empty")
        if not self.narrow_criteria:
            raise ValueError("narrow_criteria cannot be empty")

        # Check if using grouped search (dict format)
        broad_is_dict = isinstance(self.broad_criteria, dict)
        narrow_is_dict = isinstance(self.narrow_criteria, dict)

        # Validate both criteria have same type (both dict or both non-dict)
        if broad_is_dict != narrow_is_dict:
            raise ValueError(
                "broad_criteria and narrow_criteria must both be dict (for grouped search) "
                "or both be list/string (for standard search). "
                f"Got broad_criteria type: {type(self.broad_criteria).__name__}, "
                f"narrow_criteria type: {type(self.narrow_criteria).__name__}"
            )

        # If dict format, validate matching keys
        if broad_is_dict:
            broad_keys = set(self.broad_criteria.keys())
            narrow_keys = set(self.narrow_criteria.keys())
            if broad_keys != narrow_keys:
                raise ValueError(
                    "broad_criteria and narrow_criteria must have matching group keys. "
                    f"broad has: {sorted(broad_keys)}, narrow has: {sorted(narrow_keys)}"
                )
            # Use grouped search workflow
            return self._apply_grouped(db, name_column, start_date, end_date)

        # Step 1: Broad search
        broad_results = db.search_by_device_names(
            self.broad_criteria,
            start_date=start_date,
            end_date=end_date,
            deduplicate_events=True
        )

        # Step 2: Narrow search
        narrow_results = db.search_by_device_names(
            self.narrow_criteria,
            start_date=start_date,
            end_date=end_date,
            deduplicate_events=True
        )

        # Step 3: Calculate difference (reports in broad but not narrow)
        narrow_keys = set(narrow_results['MDR_REPORT_KEY'].astype(str))
        needs_review = broad_results[
            ~broad_results['MDR_REPORT_KEY'].astype(str).isin(narrow_keys)
        ].copy()

        # Step 4: Apply exclusion patterns
        excluded_list = []
        if self.exclusion_patterns:
            # Check concatenated name column if available
            if name_column in needs_review.columns:
                for pattern in self.exclusion_patterns:
                    mask = needs_review[name_column].astype(str).str.contains(
                        pattern, case=False, na=False
                    )
                    excluded_list.append(needs_review[mask])
                    needs_review = needs_review[~mask]
            else:
                # Fallback: check BRAND_NAME, GENERIC_NAME, MANUFACTURER_D_NAME
                for pattern in self.exclusion_patterns:
                    mask = pd.Series([False] * len(needs_review), index=needs_review.index)

                    for col in ['BRAND_NAME', 'GENERIC_NAME', 'MANUFACTURER_D_NAME']:
                        if col in needs_review.columns:
                            mask |= needs_review[col].astype(str).str.contains(
                                pattern, case=False, na=False
                            )

                    excluded_list.append(needs_review[mask])
                    needs_review = needs_review[~mask]

        # Combine all excluded reports
        if excluded_list:
            excluded = pd.concat(excluded_list, ignore_index=True)
        else:
            excluded = pd.DataFrame(columns=broad_results.columns)

        # Step 5: Apply manual overrides
        included = narrow_results.copy()

        # Add manually included reports
        if self.inclusion_overrides:
            inclusion_keys = set(str(k) for k in self.inclusion_overrides)
            manual_includes = needs_review[
                needs_review['MDR_REPORT_KEY'].astype(str).isin(inclusion_keys)
            ]
            if len(manual_includes) > 0:
                included = pd.concat([included, manual_includes], ignore_index=True)
                needs_review = needs_review[
                    ~needs_review['MDR_REPORT_KEY'].astype(str).isin(inclusion_keys)
                ]

        # Move manually excluded reports to excluded
        if self.exclusion_overrides:
            exclusion_keys = set(str(k) for k in self.exclusion_overrides)

            # Check both needs_review and included for manual exclusions
            manual_excludes_from_review = needs_review[
                needs_review['MDR_REPORT_KEY'].astype(str).isin(exclusion_keys)
            ]
            manual_excludes_from_included = included[
                included['MDR_REPORT_KEY'].astype(str).isin(exclusion_keys)
            ]

            if len(manual_excludes_from_review) > 0:
                excluded = pd.concat([excluded, manual_excludes_from_review], ignore_index=True)
                needs_review = needs_review[
                    ~needs_review['MDR_REPORT_KEY'].astype(str).isin(exclusion_keys)
                ]

            if len(manual_excludes_from_included) > 0:
                excluded = pd.concat([excluded, manual_excludes_from_included], ignore_index=True)
                included = included[
                    ~included['MDR_REPORT_KEY'].astype(str).isin(exclusion_keys)
                ]

        return included, excluded, needs_review

    def _apply_grouped(self, db, name_column: str,
                       start_date: Optional[str], end_date: Optional[str]) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """
        Apply search strategy with grouped dict criteria.

        Internal method for handling dict-based (grouped) search criteria.
        Preserves search_group column throughout the PRISMA workflow.

        Args:
            db: MaudeDatabase instance
            name_column: Column for substring matching
            start_date: Optional start date
            end_date: Optional end date

        Returns:
            Tuple of (included, excluded, needs_review) DataFrames with search_group column
        """
        # Step 1 & 2: Broad and narrow searches (search_by_device_names handles dict input)
        broad_results = db.search_by_device_names(
            self.broad_criteria,
            start_date=start_date,
            end_date=end_date,
            deduplicate_events=True
        )

        narrow_results = db.search_by_device_names(
            self.narrow_criteria,
            start_date=start_date,
            end_date=end_date,
            deduplicate_events=True
        )

        # Step 3: Calculate difference (preserves search_group column)
        narrow_keys = set(narrow_results['MDR_REPORT_KEY'].astype(str))
        needs_review = broad_results[
            ~broad_results['MDR_REPORT_KEY'].astype(str).isin(narrow_keys)
        ].copy()

        # Step 4: Apply exclusion patterns (same logic as list mode, search_group preserved)
        excluded_list = []
        if self.exclusion_patterns:
            # Check concatenated name column if available
            if name_column in needs_review.columns:
                for pattern in self.exclusion_patterns:
                    mask = needs_review[name_column].astype(str).str.contains(
                        pattern, case=False, na=False
                    )
                    excluded_list.append(needs_review[mask])
                    needs_review = needs_review[~mask]
            else:
                # Fallback: check BRAND_NAME, GENERIC_NAME, MANUFACTURER_D_NAME
                for pattern in self.exclusion_patterns:
                    mask = pd.Series([False] * len(needs_review), index=needs_review.index)

                    for col in ['BRAND_NAME', 'GENERIC_NAME', 'MANUFACTURER_D_NAME']:
                        if col in needs_review.columns:
                            mask |= needs_review[col].astype(str).str.contains(
                                pattern, case=False, na=False
                            )

                    excluded_list.append(needs_review[mask])
                    needs_review = needs_review[~mask]

        # Combine all excluded reports (preserves search_group)
        if excluded_list:
            excluded = pd.concat(excluded_list, ignore_index=True)
        else:
            # Empty DataFrame with same columns as broad_results (including search_group)
            excluded = pd.DataFrame(columns=broad_results.columns)

        # Step 5: Apply manual overrides (search_group inherited from source DataFrame)
        included = narrow_results.copy()

        # Add manually included reports
        if self.inclusion_overrides:
            inclusion_keys = set(str(k) for k in self.inclusion_overrides)
            manual_includes = needs_review[
                needs_review['MDR_REPORT_KEY'].astype(str).isin(inclusion_keys)
            ]
            if len(manual_includes) > 0:
                included = pd.concat([included, manual_includes], ignore_index=True)
                needs_review = needs_review[
                    ~needs_review['MDR_REPORT_KEY'].astype(str).isin(inclusion_keys)
                ]

        # Move manually excluded reports to excluded
        if self.exclusion_overrides:
            exclusion_keys = set(str(k) for k in self.exclusion_overrides)

            # Check both needs_review and included for manual exclusions
            manual_excludes_from_review = needs_review[
                needs_review['MDR_REPORT_KEY'].astype(str).isin(exclusion_keys)
            ]
            manual_excludes_from_included = included[
                included['MDR_REPORT_KEY'].astype(str).isin(exclusion_keys)
            ]

            if len(manual_excludes_from_review) > 0:
                excluded = pd.concat([excluded, manual_excludes_from_review], ignore_index=True)
                needs_review = needs_review[
                    ~needs_review['MDR_REPORT_KEY'].astype(str).isin(exclusion_keys)
                ]

            if len(manual_excludes_from_included) > 0:
                excluded = pd.concat([excluded, manual_excludes_from_included], ignore_index=True)
                included = included[
                    ~included['MDR_REPORT_KEY'].astype(str).isin(exclusion_keys)
                ]

        return included, excluded, needs_review

    def to_yaml(self, path: Optional[Path] = None) -> str:
        """
        Export strategy to YAML format for version control.

        Args:
            path: Optional file path to write YAML. If None, returns string.

        Returns:
            YAML string representation
        """
        # Convert dataclass to dict
        data = asdict(self)

        # Convert datetime objects to ISO 8601 strings for YAML serialization
        data['created_at'] = self.created_at.isoformat()
        data['updated_at'] = self.updated_at.isoformat()

        # Generate YAML string
        yaml_str = yaml.dump(
            data,
            default_flow_style=False,
            allow_unicode=True,
            sort_keys=False
        )

        # Write to file if path provided
        if path is not None:
            path = Path(path)
            path.parent.mkdir(parents=True, exist_ok=True)
            with open(path, 'w', encoding='utf-8') as f:
                f.write(yaml_str)

        return yaml_str

    @classmethod
    def from_yaml(cls, path: Path) -> "DeviceSearchStrategy":
        """
        Load strategy from YAML file.

        Args:
            path: Path to YAML file

        Returns:
            DeviceSearchStrategy instance

        Raises:
            FileNotFoundError: If YAML file doesn't exist
            ValueError: If YAML format is invalid
        """
        path = Path(path)
        if not path.exists():
            raise FileNotFoundError(f"Strategy file not found: {path}")

        try:
            with open(path, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f)

            # Convert datetime strings back to datetime objects
            if 'created_at' in data and isinstance(data['created_at'], str):
                data['created_at'] = datetime.fromisoformat(data['created_at'])
            if 'updated_at' in data and isinstance(data['updated_at'], str):
                data['updated_at'] = datetime.fromisoformat(data['updated_at'])

            return cls(**data)

        except yaml.YAMLError as e:
            raise ValueError(f"Failed to parse YAML file: {e}")
        except TypeError as e:
            raise ValueError(f"Invalid YAML format: {e}")

    def add_manual_decision(self, mdr_key: str, decision: str, reason: str = ""):
        """
        Record a manual inclusion/exclusion decision.

        Args:
            mdr_key: MDR_REPORT_KEY as string
            decision: "include" or "exclude"
            reason: Brief explanation for documentation (not stored in strategy)

        Raises:
            ValueError: If decision is not "include" or "exclude"

        Note:
            Updates inclusion_overrides or exclusion_overrides list.
            Updates updated_at timestamp.
            For detailed decision tracking, use AdjudicationLog.
        """
        if decision not in ("include", "exclude"):
            raise ValueError(f"decision must be 'include' or 'exclude', got: {decision}")

        mdr_key = str(mdr_key)

        if decision == "include":
            if mdr_key not in self.inclusion_overrides:
                self.inclusion_overrides.append(mdr_key)
            # Remove from exclusions if present
            if mdr_key in self.exclusion_overrides:
                self.exclusion_overrides.remove(mdr_key)
        else:  # exclude
            if mdr_key not in self.exclusion_overrides:
                self.exclusion_overrides.append(mdr_key)
            # Remove from inclusions if present
            if mdr_key in self.inclusion_overrides:
                self.inclusion_overrides.remove(mdr_key)

        self.updated_at = datetime.now()

    def sync_from_adjudication(self, log):
        """
        Sync manual decisions from an AdjudicationLog into this strategy.

        This method updates the strategy's inclusion_overrides and exclusion_overrides
        to match all decisions recorded in the adjudication log. This is the recommended
        workflow: track detailed decisions in AdjudicationLog (with reasons, reviewers,
        dates), then sync to DeviceSearchStrategy for reproducible application.

        Args:
            log: AdjudicationLog instance containing manual decisions

        Returns:
            dict: Summary with keys:
                - 'inclusions_added': Number of inclusion overrides synced
                - 'exclusions_added': Number of exclusion overrides synced
                - 'total_synced': Total decisions synced

        Example:
            # Create adjudication log and add decisions
            log = AdjudicationLog('adjudication/venous_stent.csv')
            log.add('1234567', 'include', 'Matches criteria', 'Jake')
            log.add('7654321', 'exclude', 'False positive', 'Jake')
            log.to_csv()

            # Load strategy and sync decisions
            strategy = DeviceSearchStrategy.from_yaml('strategies/venous_stent_v1.yaml')
            summary = strategy.sync_from_adjudication(log)
            print(f"Synced {summary['total_synced']} decisions")

            # Save updated strategy
            strategy.to_yaml('strategies/venous_stent_v1.yaml')

        Note:
            This replaces (not appends) the existing inclusion/exclusion overrides.
            The detailed audit trail remains in the AdjudicationLog CSV file.
        """
        from .adjudication import AdjudicationLog

        if not isinstance(log, AdjudicationLog):
            raise ValueError(f"log must be an AdjudicationLog instance, got {type(log).__name__}")

        # Get all decisions from log
        inclusion_keys = log.get_inclusion_keys()
        exclusion_keys = log.get_exclusion_keys()

        # Replace override lists (convert sets to lists)
        self.inclusion_overrides = sorted(list(inclusion_keys))
        self.exclusion_overrides = sorted(list(exclusion_keys))

        # Update timestamp
        self.updated_at = datetime.now()

        return {
            'inclusions_added': len(inclusion_keys),
            'exclusions_added': len(exclusion_keys),
            'total_synced': len(inclusion_keys) + len(exclusion_keys)
        }

    def get_prisma_counts(self, included_df: pd.DataFrame, excluded_df: pd.DataFrame,
                          needs_review_df: pd.DataFrame) -> Dict[str, int]:
        """
        Generate counts for PRISMA flow diagram reporting.

        See PRISMA 2020 Item 16a for reporting requirements.

        Args:
            included_df: DataFrame of included reports
            excluded_df: DataFrame of excluded reports
            needs_review_df: DataFrame of reports needing review

        Returns:
            Dictionary with keys:
            - broad_matches: Count from broad search
            - narrow_matches: Count from narrow search
            - needs_manual_review: Count needing adjudication
            - manual_inclusions: Count of inclusion overrides
            - manual_exclusions: Count of exclusion overrides
            - final_included: Final count after all filters
            - final_excluded: Final excluded count
            - excluded_by_patterns: Count excluded by pattern matching
        """
        # Calculate counts
        broad_matches = len(included_df) + len(excluded_df) + len(needs_review_df)
        narrow_matches = len(included_df)  # Before manual adjustments

        # Count manual overrides applied
        manual_inclusions = len(self.inclusion_overrides) if self.inclusion_overrides else 0
        manual_exclusions = len(self.exclusion_overrides) if self.exclusion_overrides else 0

        # Pattern-based exclusions (rough estimate from excluded_df)
        excluded_by_patterns = len(excluded_df) - manual_exclusions

        return {
            'broad_matches': broad_matches,
            'narrow_matches': narrow_matches - manual_exclusions,  # Narrow before manual exclusions
            'needs_manual_review': len(needs_review_df),
            'manual_inclusions': manual_inclusions,
            'manual_exclusions': manual_exclusions,
            'final_included': len(included_df),
            'final_excluded': len(excluded_df),
            'excluded_by_patterns': max(0, excluded_by_patterns)
        }
